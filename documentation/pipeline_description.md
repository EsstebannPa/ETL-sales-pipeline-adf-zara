# Pipeline ETL ZARA en ADF: Descripci√≥n üîÅ
#
#
## Objetivo del flujo

El Pipeline a continuaci√≥n tiene como objetivo simular un proceso de transformaci√≥n de datos de ventas utilizando Azure Data Factory, moviendo los datos desde un almacenamiento Data Lake, con Azure Blob Storage `raw-zone` hacia una zona limpia `clean-zone`, a trav√©s de un flujo de transformaci√≥n visual (*Mapping Data Flow*) en ADF Studio.
#
---

## Elementos del Pipeline üõ†

### Origen (`source1`) üìÇ
- Conectado a un archivo `.csv` almacenado en Azure Blob Storage (`raw-zone`).
- Dataset: `zara_sales_june.csv`
- Estructura: columnas como `product_id`, `product_name`, `price`, `sales_volume`...

### Selecci√≥n (`columnstransform1`) ‚úî
- Se realiza la selecci√≥n de campos a utilizar desde source1
- Se relizan los cambios de tipificaci√≥n de los campos.
- Este proceso permite estandarizar y mejorar la estructura de los datos.

### Transformaci√≥n (`addcolumn`) ‚ôª
- Se a√±ade una nueva columna calculada derivada o calculada
 ```revenue = price * sales_volume```
- Esta operaci√≥n permite simular una m√©trica clave de negocio

### Destino (`sink1`) üì¶
- Se determina un nombre para el archivo limpio resultante, y se almacena en el         contenedor `clean-zone` de Azure Blob Storage.
- Formato de salida: `.csv` delimitado por punto y coma `;`
- Ahora el archivo contiene los mismos registros de entrada, pero con una mejor calidad en sus columnas y su aportaci√≥n informativa gracias a la nueva columna `revenue`.

---

## Arquitectura visual del flujo üó∫
#

```plaintext
                        Azure Blob Storage (raw-zone)
                                     ‚Üì
                                  source1
                                     ‚Üì
                             columnstransfrom1
                                     ‚Üì
                                 addcolumn
                                     ‚Üì
                                   sink1
                                     ‚Üì
                           Azure Blob Storage (clean)
```
---
# Paso a paso üßó‚Äç‚ôÇÔ∏è
Una vez se tenga la cuenta azure debidamente activa, con los servicios necesarios, los 2 recursos creados: `almacenamiento blob` , `azure data factory (ADF)`, una cuenta de almacenamiento y una instancia de ADF creadas, se puede proceder con los siguientes pasos:

## 1. Creaci√≥n de contenedores

Gracias al servicio `Azure Blob Storage`, tenemos la posibilidad de crear un almacenamiento de datos del tipo Data Lake, en cu√°l crearemos un contenedor `raw-zone` y otro `clean-zone`, este √∫ltimo para una vez tengamos la informaci√≥n lista, poder acceder a ella.

Tambi√©n, se proceder√° a cargar de manera manual, el archivo csv en la raw-zone.

![Configuraci√≥n de zonas]()
![Configuraci√≥n de zonas]()

## 2. Creaci√≥n de Servicios Vinculados

Se accede al Estudio de ADF, y all√≠ en el apartado de `Administrar`, se agrega un nuevo servicio vinculado, donde haremos una conexi√≥n hacia la cuenta de almacenamiento o servicio de Azure Blob Storage, de esta manera, podemos trabajar con la data que all√≠ se almacena, desde nuestro recurso ADF.

![Configuraci√≥n de servicios vinculados]()
![Configuraci√≥n de servicios vinculados]()
![Configuraci√≥n de servicios vinculados]()

## 3.Creaci√≥n de Datasets dentro de ADF

En la secci√≥n de `Autor`, se selecciona la opci√≥n de crear un nuevo Dataset a partir de nuestro servicio vinculado anteriormente `Azure Blob Storage`, se selecciona el formato para archivos, en este caso `DelimitedText` y se configura el resto de par√°metros, nombre: ds-raw-data. Ojo!, el delimitador debe estar con `;`. Repetir el mismo proceso pero ahora con el Dataset de almacenamiento limpio: `ds-clean-data`.

![Configuraci√≥n de Datasets en ADF Studio]()
![Configuraci√≥n de Datasets en ADF Studio]()
![Configuraci√≥n de Datasets en ADF Studio]()
![Configuraci√≥n de Datasets en ADF Studio]()

## 4.Creaci√≥n de Flujo de datos

[Leer documentaci√≥n completa de Flujo de datos ADF](ruta/al/otro/archivo.md)

## 5.Creaci√≥n de Pipeline o Canalizador

En la misma secci√≥n `Autor`, se selecciona el recurso de f√°brica `Canalizaciones` y se crea una `Nueva Canalizaci√≥n`, una vez creada, la seleccionaremos y en el panel de actividades seleccionaremos `Mover y Transformar` > `Flujo de datos`, all√≠ configuraremos el nuevo flujo de datos que hemos creado y el resto de par√°metros opcionales.

![Configuraci√≥n de Datasets en ADF Studio]()
![Configuraci√≥n de Datasets en ADF Studio]()




## 6.Realizar Debug y Publicaci√≥n

Una Vez realizados todos los pasos, procederemos a Depurar, Validar y Publicar los cambios de ADF, 

## 7.Ejecuci√≥n (Desencadenador)
Una vez se haya validado y publicado, procedemos a ejecutar el pipeline manualmente, de la siguiente manera `Pipeline Creado` > `Agregar Desencadenador` > `Desencadenar Ahora`. El Pipeline se ejecutar√°, todo su proceso se observar√° en el en el side panel, apartado de `Monitoreo`. 



## 8.Validaci√≥n 

Una vez la ejecuci√≥n haya finalizado sin problemas, podremos validar yendo nuevamente al recurso de almacenamiento, cuenta de almacenamiento, y all√≠, `clean-zone` , validamos y procedemos a hacer los an√°lisis necesarios. 

#
#
#### _Autor: Esteban Parrado_
