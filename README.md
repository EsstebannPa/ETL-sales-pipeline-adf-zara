# Pipeline ETL: Automatizado con Azure Data Factory (Ventas Zara) ðŸ”
#
#
## DescripciÃ³n ðŸ“ƒ

El presente proyecto simula un **Pipeline ETL automatizado** utilizando **Azure Data Factory (ADF)**. El proyecto se basa en procesar datos de ventas ficticias de Zara, desde archivos crudos en **Azure Blob Storage** hasta una zona limpia y lista para su posterior anÃ¡lisis.

Esto representa una situaciÃ³n comÃºn en empresas de retail, donde los datos de ventas se reciben diariamente en formatos tales como CSV, y deben ser procesados, transformados y almacenados para su posterior anÃ¡lisis.
#
## Herramientas utilizadas ðŸ› 
#

| Herramienta | PropÃ³sito |
| ------ | ------ |
| `Azure Data Factory (ADF)` | OrquestaciÃ³n del pipeline y transformaciÃ³n de datos |
| `Azure Blob Storage` | Almacenamiento de datos en zonas _raw_ y _clean_ |
| `Python 3.13 (Pandas)` | AnÃ¡lisis exploratorio inicial del dataset |
| [Draw.io](https://www.drawio.com/) | DiseÃ±o del diagrama de arquitectura |

#
## Zonas en Azure Blob Storage ðŸ“

- **Raw Zone**: Archivos CSV originales cargados manualmente.
- **Clean Zone**: Archivos procesados o limpios generados por el Pipeline en ADF.
#
## Flujo del Pipeline ðŸ§µ

1. ðŸ’» Carga inicial del archivo `zara_sales_june.csv` a Azure Blob Storage.
2. ðŸ§© ADF ejecuta el flujo de datos, tomando como source `ds-raw-data` hacia el sick o receptor `ds-clean-data`.
3. ðŸ“ El pipeline transforma y agrega una nueva columna derivada llamada `revenue` (ganancia).
4. ðŸ“¦ Se guarda un nuevo archivo en `ds-clean-data` y posteriormente en `clean-zone`, (Azure Blob), ahora limpio y listo para su anÃ¡lisis.

##  Estructura del proyecto ðŸ“š
#
```## Estructura del Proyecto
âž¡ ETL-sales-pipeline-adf-zara
    * architecture/
        * ArchivosReadme.md
    * azure/
        * logs/
            - ResultadosDeEjecuciÃ³n.txt
    * data/
        * raw/
            - zara_sales_june.csv
        * clean/
    * documentation/
        * DocumentaciÃ³n del proyecto
        * screenshots/
            - CapturasDePantalla.png
    * README.md
    * requirements.md
```

# ReflexiÃ³n final ðŸ¤”

A travÃ©s del proyecto, mi capacidad de entendimiento y anÃ¡isis del mundo automatizado de los Pipelines, aumentÃ³ en gran medida, aprendÃ­ a relizar una creaciÃ³n completa de un Pipeline ETL en Azure Data Factory, conociendo cada paso, cada conexiÃ³n y lo mÃ¡s importante, el propÃ³sito general y de cada uno de ellos. Aprendi tambiÃ©n la configuraciÃ³n bÃ¡sica de una cuenta de almacenamiento y sus contenedores con el servicio Azure Blob Storage, como tambiÃ©n, la documentaciÃ³n clara de todo el proyecto. 

_Gracias por leer.._
#### _Autor: Esteban Parrado_
#
#
#
#
>â€œLa tecnologÃ­a no es nada. Lo importante es que tengas fe en la gente, que sean bÃ¡sicamente buenas e inteligentes, y si les das herramientas, harÃ¡n cosas maravillosas con ellasâ€
>-Steve Jobs.
